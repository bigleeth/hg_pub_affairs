import requests
from bs4 import BeautifulSoup
import json
import pandas as pd
from datetime import datetime
from zoneinfo import ZoneInfo

### =============== êµ­íšŒì˜ì› ì •ë³´ ìˆ˜ì§‘ ===============
cookies_members = {
    '_fwb': '224GIbJbv3W2VKbiHHpHIKa.1736910833680',
    '_ga': 'GA1.1.1553764147.1736910835',
    'PCID': '756e904c-0b66-a4a7-6836-589756f10a6e-1736910838628',
    'JSESSIONID': 'your_valid_session',
    'PHAROSVISITOR': '00006eb50196285f2f296cc80ac965a6',
}
headers_members = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9',
    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
    'Connection': 'keep-alive'
}
members = [
    ("ê¹€ì˜ì§„", "KIMYOUNGJIN"),
    ("ì •íƒœí˜¸", "JUNGTAEHO"),
    ("ê¹€ì˜í™˜", "KIMYOUNGWHAN"),
    ("ê¹€íƒœë…„", "KIMTAENYEON"),
    ("ë°•í™ê·¼", "PARKHONGKEUN"),
    ("ë°•ë¯¼ê·œ", "PARKMINKYU"),
    ("ì•ˆê·œë°±", "AHNGYUBACK"),
    ("ì•ˆë„ê±¸", "AHNDOGEOL"),
    ("ì˜¤ê¸°í˜•", "OHGIHYOUNG"),
    ("ì´ì†Œì˜", "LEESOYOUNG"),
    ("ì •ì„±í˜¸", "JUNGSUNGHO"),
    ("ì •ì¼ì˜", "CHUNGILYOUNG"),
    ("ì¡°ìŠ¹ë˜", "JOSEOUNGLAE"),
    ("ì§„ì„±ì¤€", "JINSUNGJOON"),
    ("ì†¡ì–¸ì„", "SONGEONSEOG"),
    ("ë°•ìˆ˜ì˜", "PARKSOOYOUNG"),
    ("ë°•ëŒ€ì¶œ", "PARKDAECHUL"),
    ("ë°•ì„±í›ˆ", "PARKSUNGHOON"),
    ("ìœ ìƒë²”", "YOOSANGBUM"),
    ("ìœ¤ì˜ì„", "YOONYOUNGSEOK"),
    ("ì´ì¸ì„ ", "LEEINSEON"),
    ("ì„ì´ì", "LIMLEEJA"),
    ("ìµœì€ì„", "CHOIEUNSEOK"),
    ("ì°¨ê·œê·¼", "CHAGYUGEUN"),
    ("ì²œí•˜ëŒ", "CHUNHARAM"),
    ("ìµœê¸°ìƒ", "CHOIKISANG"),
    ("ê¶Œì˜ì„¸", "KWONYOUNGSE"),

]

# ì •ë‹¹ ì •ë³´ ë§¤í•‘
party_mapping = {
    "ì •íƒœí˜¸": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ê¹€ì˜ì§„": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ê¹€ì˜í™˜": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ê¹€íƒœë…„": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ë°•í™ê·¼": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ë°•ë¯¼ê·œ": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ì•ˆê·œë°±": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ì•ˆë„ê±¸": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ì˜¤ê¸°í˜•": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ì´ì†Œì˜": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ì •ì„±í˜¸": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ì •ì¼ì˜": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ì¡°ìŠ¹ë˜": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ì§„ì„±ì¤€": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ìµœê¸°ìƒ": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ì†¡ì–¸ì„": "êµ­ë¯¼ì˜í˜",
    "ë°•ìˆ˜ì˜": "êµ­ë¯¼ì˜í˜",
    "ë°•ëŒ€ì¶œ": "êµ­ë¯¼ì˜í˜",
    "ë°•ì„±í›ˆ": "êµ­ë¯¼ì˜í˜",
    "ìœ ìƒë²”": "êµ­ë¯¼ì˜í˜",
    "ìœ¤ì˜ì„": "êµ­ë¯¼ì˜í˜",
    "ì´ì¸ì„ ": "êµ­ë¯¼ì˜í˜",
    "ì„ì´ì": "êµ­ë¯¼ì˜í˜",
    "ìµœì€ì„": "êµ­ë¯¼ì˜í˜",
    "ê¶Œì˜ì„¸": "êµ­ë¯¼ì˜í˜",
    "ì°¨ê·œê·¼": "ì¡°êµ­í˜ì‹ ë‹¹",
    "ì²œí•˜ëŒ": "ê°œí˜ì‹ ë‹¹"
}




def extract_member_data(soup, name, member_id, response):
    try:
        name_el = soup.find('span', class_='sr-only')
        name = name_el.text.strip() if name_el else name
        party = party_mapping.get(name, "ì •ë³´ ì—†ìŒ")
        election_count, district, committee = "ì •ë³´ ì—†ìŒ", "ì •ë³´ ì—†ìŒ", "ì •ë³´ ì—†ìŒ"
        for dt in soup.find_all('dt'):
            if dt.text == 'ë‹¹ì„ íšŸìˆ˜':
                election_count = dt.find_next('dd').text.strip()[:2]
            elif dt.text == 'ì„ ê±°êµ¬':
                district = dt.find_next('dd').text.strip()
            elif dt.text == 'ì†Œì†ìœ„ì›íšŒ':
                committee = dt.find_next('dd').text.strip()
        chief, senior, secretary = [], [], []
        for li in soup.find_all('li'):
            title = li.find('dt')
            value = li.find('dd')
            if not title or not value:
                continue
            role = title.text.strip()
            names = [n.strip() for n in value.text.split(',')]
            if 'ë³´ì¢Œê´€' in role:
                chief = names
            elif 'ì„ ì„ë¹„ì„œê´€' in role:
                senior = names
            elif 'ë¹„ì„œê´€' in role:
                secretary = names
        return {
            "êµ­íšŒì˜ì›": {
                "ì´ë¦„": name,
                "ì •ë‹¹": party,
                "ë‹¹ì„ íšŸìˆ˜": election_count,
                "ì„ ê±°êµ¬": district,
                "ì†Œì†ìœ„ì›íšŒ": committee
            },
            "ë³´ì¢Œê´€": chief,
            "ì„ ì„ë¹„ì„œê´€": senior,
            "ë¹„ì„œê´€": secretary,
            "ë©”íƒ€ë°ì´í„°": {
                "url": f"https://www.assembly.go.kr/members/22nd/{member_id}",
                "status_code": response.status_code,
                "ìˆ˜ì§‘ì¼ì‹œ": datetime.now(ZoneInfo("Asia/Seoul")).strftime("%Y-%m-%d %H:%M:%S")
            }
        }
    except Exception as e:
        return {
            "êµ­íšŒì˜ì›": {
                "ì´ë¦„": name,
                "ì •ë‹¹": party_mapping.get(name, "ì •ë³´ ì—†ìŒ"),
                "ë‹¹ì„ íšŸìˆ˜": "ì •ë³´ ì—†ìŒ",
                "ì„ ê±°êµ¬": "ì •ë³´ ì—†ìŒ",
                "ì†Œì†ìœ„ì›íšŒ": "ì •ë³´ ì—†ìŒ"
            },
            "ë³´ì¢Œê´€": [], "ì„ ì„ë¹„ì„œê´€": [], "ë¹„ì„œê´€": [],
            "ë©”íƒ€ë°ì´í„°": {
                "url": f"https://www.assembly.go.kr/members/22nd/{member_id}",
                "status_code": 500,
                "ìˆ˜ì§‘ì¼ì‹œ": datetime.now(ZoneInfo("Asia/Seoul")).strftime("%Y-%m-%d %H:%M:%S")
            }
        }

all_member_data = []
for name, member_id in members:
    try:
        url = f'https://www.assembly.go.kr/members/22nd/{member_id}'
        response = requests.get(url, cookies=cookies_members, headers=headers_members)
        soup = BeautifulSoup(response.text, 'html.parser')
        data = extract_member_data(soup, name, member_id, response)
        all_member_data.append(data)
    except Exception as e:
        print(f"[ERROR] {name} failed: {e}")

with open('assembly_member_data.json', 'w', encoding='utf-8') as f:
    json.dump(all_member_data, f, ensure_ascii=False, indent=4)
print("âœ… êµ­íšŒì˜ì› ì •ë³´ ì €ì¥ ì™„ë£Œ")


# ### =============== ì˜ì•ˆ ì •ë³´ ìˆ˜ì§‘ ===============
import json
import re
import unicodedata
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from zoneinfo import ZoneInfo

SEARCH_URL = "https://likms.assembly.go.kr/bill/bi/bill/sch/detailedSchPage.do"

def normalize_bill_title(title: str) -> str:
    """
    - NFKC ì •ê·œí™” (íŠ¹ìˆ˜ ê³µë°±/ë¬¸ì ë³€í˜• ë°©ì§€)
    - ê´„í˜¸ ì´í›„ ì œê±°: "ë²•ë¥ ì•ˆ(â—‹â—‹ì˜ì› ë“±)" -> "ë²•ë¥ ì•ˆ"
    - ì–‘ë ê³µë°± ì œê±°
    """
    if not title:
        return ""
    t = unicodedata.normalize("NFKC", title).strip()
    t = re.split(r"\s*\(", t, maxsplit=1)[0].strip()
    return t

def collect_bill_info(bill_name: str):
    # ìƒì„¸ê²€ìƒ‰(.detailedSchPage) ëŒ€ì‹  ê°„í¸ê²€ìƒ‰(.billSimpleSearch) ì‚¬ìš©
    # ê°„í¸ê²€ìƒ‰ì´ í‚¤ì›Œë“œ ë§¤ì¹­(GETë°©ì‹)ì— í›¨ì”¬ ì•ˆì •ì ì…ë‹ˆë‹¤.
    URL = "https://likms.assembly.go.kr/bill/bi/bill/sch/billSimpleSearch.do"
    
    session = requests.Session()
    session.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Referer": URL,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "ko-KR,ko;q=0.9",
    })

    # ê°„í¸ ê²€ìƒ‰ íŒŒë¼ë¯¸í„°: searchStr
    params = {
        "searchStr": bill_name,
        "searchGubun": "1"  # 1: ì˜ì•ˆëª… ê²€ìƒ‰
    }

    try:
        r = session.get(URL, params=params, timeout=30)
        r.raise_for_status()
    except Exception as e:
        print(f"âš ï¸ [ìš”ì²­ ì‹¤íŒ¨] {bill_name}: {e}")
        return []

    soup = BeautifulSoup(r.text, "html.parser")

    # ê°„í¸ê²€ìƒ‰ ê²°ê³¼ í…Œì´ë¸”ì˜ í–‰ ì„ íƒì (tableDisplay01 í´ë˜ìŠ¤ ë‚´ë¶€ì˜ tbody tr)
    rows = soup.select("div.tableDisplay01 > table > tbody > tr")

    # ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ ì²˜ë¦¬
    if not rows:
        print(f"â„¹ï¸ [{bill_name}] ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []
        
    # 'ê²€ìƒ‰ëœ ìë£Œê°€ ì—†ìŠµë‹ˆë‹¤' í…ìŠ¤íŠ¸ê°€ ìˆëŠ” í–‰ì´ ë‚˜ì˜¤ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    if len(rows) == 1 and "ìë£Œê°€ ì—†ìŠµë‹ˆë‹¤" in rows[0].get_text():
        print(f"â„¹ï¸ [{bill_name}] ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤ (ì‚¬ì´íŠ¸ ì‘ë‹µ).")
        return []

    target = normalize_bill_title(bill_name)
    print(f"ğŸ” [{bill_name}] ê²€ìƒ‰ ì™„ë£Œ (ë°œê²¬: {len(rows)}ê±´)")

    bills = []
    
    for row in rows:
        tds = row.find_all("td")
        # ê°„í¸ê²€ìƒ‰ ê²°ê³¼ í…Œì´ë¸” ì»¬ëŸ¼ ê°œìˆ˜ í™•ì¸ (ë³´í†µ 6~7ê°œ)
        if len(tds) < 5:
            continue

        # ì˜ì•ˆë²ˆí˜¸ (1ë²ˆì§¸ ì»¬ëŸ¼)
        bill_no = tds[0].get_text(strip=True)

        # ì˜ì•ˆëª… (2ë²ˆì§¸ ì»¬ëŸ¼)
        title_td = tds[1]
        bill_title_full = title_td.get_text(" ", strip=True)
        
        # billId ì¶”ì¶œ (onclick ì´ë²¤íŠ¸ íŒŒì‹±)
        bill_id = ""
        a = title_td.find("a")
        if a and a.has_attr("onclick"):
            # ì˜ˆ: fGoDetail('PRC_A1B2...', 'billSimpleSearch')
            m = re.search(r"fGoDetail\(['\"]([^'\"]+)['\"]", a["onclick"])
            if m:
                bill_id = m.group(1)

        # ì œì•ˆì (3ë²ˆì§¸ ì»¬ëŸ¼)
        proposer = tds[2].get_text(strip=True)
        
        # ì œì•ˆì¼ì (4ë²ˆì§¸ ì»¬ëŸ¼)
        propose_date = tds[3].get_text(strip=True)
        
        # ì‹¬ì‚¬ì§„í–‰ìƒíƒœ (6ë²ˆì§¸ ì»¬ëŸ¼) - í…Œì´ë¸” êµ¬ì¡°ì— ë”°ë¼ ì¸ë±ìŠ¤ í™•ì¸ í•„ìš”
        # ë³´í†µ: ë²ˆí˜¸ | ì˜ì•ˆëª… | ì œì•ˆì | ì œì•ˆì¼ì | ì˜ê²°ì¼ì | ì˜ê²°ê²°ê³¼ | ì‹¬ì‚¬ì§„í–‰ìƒíƒœ
        # ë”°ë¼ì„œ tds[6]ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë‚˜, ì•ˆì „í•˜ê²Œ ë§¨ ë’¤ì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ í…ìŠ¤íŠ¸ ë§¤ì¹­
        status = tds[-1].get_text(strip=True) 

        # í•„í„°ë§: ê²€ìƒ‰ì–´ê°€ ì œëª©ì— í¬í•¨ë˜ëŠ”ì§€ í™•ì¸
        if bill_name.replace(" ", "") not in bill_title_full.replace(" ", ""):
            continue

        print(f"   âœ… ìˆ˜ì§‘ ì„±ê³µ: {bill_title_full}")

        bills.append({
            "ì˜ì•ˆë²ˆí˜¸": bill_no,
            "ì˜ì•ˆID": bill_id,
            "ì˜ì•ˆëª…": {
                "text": bill_title_full,
                "link": f"https://likms.assembly.go.kr/bill/bi/bill/detail.do?billId={bill_id}" if bill_id else ""
            },
            "ì œì•ˆì": proposer,
            "ì œì•ˆì¼ì": propose_date,
            "ì‹¬ì‚¬ì§„í–‰ìƒíƒœ": status,
            "ìˆ˜ì§‘ì¼ì‹œ": datetime.now(ZoneInfo("Asia/Seoul")).strftime("%Y-%m-%d %H:%M:%S"),
        })

    return bills


# âœ… ì´ ë²•ë¥ ë“¤ë§Œ ì €ì¥(ì‚¬ìš©ì ìš”êµ¬ ë°˜ì˜)
bill_names = [
    "í•œêµ­ìˆ˜ì¶œì…ì€í–‰ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ",
    "ê²½ì œì•ˆë³´ë¥¼ ìœ„í•œ ê³µê¸‰ë§ ì•ˆì •í™” ì§€ì› ê¸°ë³¸ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ",
    "ì²¨ë‹¨ì¡°ì„ ì—…ì˜ ê²½ìŸë ¥ ê°•í™” ë° ì§€ì›ì— ê´€í•œ íŠ¹ë³„ë²•ì•ˆ",
    "ê³µê³µê¸°ê´€ì˜ ìš´ì˜ì— ê´€í•œ ë²•ë¥  ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ",
    "í•œêµ­ì‚°ì—…ì€í–‰ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ",
    "2025ë…„ë„ì— ë°œí–‰í•˜ëŠ” ì²¨ë‹¨ì „ëµì‚°ì—…ê¸°ê¸ˆì±„ê¶Œì— ëŒ€í•œ êµ­ê°€ë³´ì¦ë™ì˜ì•ˆ",
    "ì¤‘ì†Œê¸°ì—…ì€í–‰ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ",
    "ì •ë¶€ì¡°ì§ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ",
    "ì‹ ìš©ë³´ì¦ê¸°ê¸ˆë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ",
    "ë™ë‚¨ê¶Œì‚°ì—…íˆ¬ìê³µì‚¬ ì„¤ë¦½ ë° ìš´ì˜ì— ê´€í•œ ë²•ë¥ ì•ˆ",
    "ì¶©ì²­ê¶Œì‚°ì—…íˆ¬ìê³µì‚¬ ì„¤ë¦½ ë° ìš´ì˜ì— ê´€í•œ ë²•ë¥ ì•ˆ",
    "ê¸°í›„ìœ„ê¸° ëŒ€ì‘ì„ ìœ„í•œ íƒ„ì†Œì¤‘ë¦½ã†ë…¹ìƒ‰ì„±ì¥ ê¸°ë³¸ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ"
]

all_bills = []
for name in bill_names:
    try:
        rows = collect_bill_info(name)
        all_bills.extend(rows)
        print(f"âœ… [{name}] {len(rows)}ê±´ ìˆ˜ì§‘")
    except Exception as e:
        print(f"âš ï¸ [{name}] ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

with open("ì˜ì•ˆì •ë³´ê²€ìƒ‰ê²°ê³¼.json", "w", encoding="utf-8") as f:
    json.dump(all_bills, f, ensure_ascii=False, indent=4)

print(f"âœ… ì˜ì•ˆ ì •ë³´ ì €ì¥ ì™„ë£Œ: {len(all_bills)}ê±´")


### =============== ì†Œìœ„ì›íšŒ ì •ë³´ ìˆ˜ì§‘ ===============
# ì„¸ì…˜ ê°ì²´ ìƒì„±
session = requests.Session()

# ë©”ì¸ í˜ì´ì§€ URL
main_url = 'https://finance.na.go.kr:444/cmmit/mem/cmmitMemList/subCmt.do?menuNo=2000014'

# í—¤ë” ì •ì˜
headers = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'ko-KR,ko;q=0.9',
    'Cache-Control': 'no-cache',
    'Connection': 'keep-alive',
    'Pragma': 'no-cache',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
}

try:
    # ë©”ì¸ í˜ì´ì§€ ìš”ì²­
    main_response = session.get(main_url, headers=headers)
    main_response.raise_for_status()

    # CSRF ë©”íƒ€ íƒœê·¸ ì¶”ì¶œ
    soup = BeautifulSoup(main_response.text, 'html.parser')
    csrf_parameter = soup.find('meta', {'name': '_csrf_parameter'})
    csrf_header = soup.find('meta', {'name': '_csrf_header'})
    csrf_token = soup.find('meta', {'name': '_csrf'})

    if not all([csrf_parameter, csrf_header, csrf_token]):
        raise ValueError("CSRF ë©”íƒ€ íƒœê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    csrf_parameter_value = csrf_parameter['content']
    csrf_header_value = csrf_header['content']
    csrf_token_value = csrf_token['content']

    # POST ìš”ì²­ í—¤ë” ì„¤ì •
    api_headers = {
        'Accept': 'application/json, text/javascript, */*; q=0.01',
        'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
        'Origin': 'https://finance.na.go.kr:444',
        'Referer': main_url,
        'User-Agent': headers['User-Agent'],
        'X-Requested-With': 'XMLHttpRequest',
        csrf_header_value: csrf_token_value
    }

    data = {
        'hgNm': '',
        'subCmtNm': '',
        'pageUnit': '9',
        'pageIndex': '',
        csrf_parameter_value: csrf_token_value
    }

    response = session.post(
        'https://finance.na.go.kr:444/cmmit/mem/cmmitMemList/getSubCmitLst.json',
        headers=api_headers,
        data=data
    )
    response.raise_for_status()
    response_data = response.json()

except requests.exceptions.RequestException as e:
    print(f"ìš”ì²­ ì‹¤íŒ¨: {e}")
    exit(1)
except json.JSONDecodeError as e:
    print(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
    print("ì‘ë‹µ ì›ë¬¸:", response.text)
    exit(1)
except Exception as e:
    print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
    exit(1)

# â—ˆì´ë¦„ â†’ ì´ë¦„(é•·)ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def parse_members(member_str):
    members = []
    for m in member_str.split(','):
        m = m.strip()
        if m.startswith('â—ˆ'):
            m = m.lstrip('â—ˆ')  # remove diamond
            if '(é•·)' not in m:
                name_part, sep, han_part = m.partition('(')
                m = f"(é•·){name_part}{sep}{han_part}"  # insert "(é•·)" before names
        members.append(m)
    return members

# ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ êµ¬ì„±
result = {}

for item in response_data.get("resultList", []):
    committee_name = item["sbcmtNm"]
    count = item["naasCnt"]
    key = f"{committee_name}({count}ì¸)"
    
    parties = {}
    if item.get("poly1NaasNm") and item.get("poly1NaasCn"):
        parties[item["poly1NaasNm"]] = parse_members(item["poly1NaasCn"])
    if item.get("poly2NaasNm") and item.get("poly2NaasCn"):
        parties[item["poly2NaasNm"]] = parse_members(item["poly2NaasCn"])
    if item.get("poly99NaasNm") and item.get("poly99NaasCn"):
        parties[item["poly99NaasNm"]] = parse_members(item["poly99NaasCn"])
    
    result[key] = parties

# ë©”íƒ€ë°ì´í„° êµ¬ì„±
metadata = {
    "ìˆ˜ì§‘ì¼ì‹œ": datetime.now(ZoneInfo("Asia/Seoul")).strftime("%Y-%m-%d %H:%M:%S"),
    "url": main_url,
    "status_code": response.status_code
}

# ìµœì¢… ê²°ê³¼ ì €ì¥
final_result = {
    "ì†Œìœ„ì›íšŒ_ì •ë³´": result,
    "ë©”íƒ€ë°ì´í„°": metadata
}

with open('ì†Œìœ„ì›íšŒì •ë³´.json', 'w', encoding='utf-8') as f:
    json.dump(final_result, f, ensure_ascii=False, indent=4)
print("âœ… ì†Œìœ„ì›íšŒ ì •ë³´ ì €ì¥ ì™„ë£Œ")









