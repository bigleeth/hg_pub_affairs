# update_json.py
# -*- coding: utf-8 -*-

import json
import re
import unicodedata
from datetime import datetime
from zoneinfo import ZoneInfo

import requests
from bs4 import BeautifulSoup


# =========================================================
# ê³µí†µ ìœ í‹¸
# =========================================================
KST = ZoneInfo("Asia/Seoul")


def now_kst_str() -> str:
    return datetime.now(KST).strftime("%Y-%m-%d %H:%M:%S")


def normalize_bill_title(title: str) -> str:
    """
    - NFKC ì •ê·œí™”
    - 'ê³„ë¥˜ì˜ì•ˆ', 'ì²˜ë¦¬ì˜ì•ˆ' ê°™ì€ ì ‘ë‘ì–´ ì œê±°
    - ê´„í˜¸ ë¸”ë¡ ì „ë¶€ ì œê±°: (ëŒ€ì•ˆ)(OOì˜ì› ë“±) ë“± ì—¬ëŸ¬ ê°œë„ ì œê±°
    - ê³µë°± ì •ë¦¬
    """
    if not title:
        return ""

    t = unicodedata.normalize("NFKC", title).strip()

    # âœ… ì ‘ë‘ì–´ ì œê±° (td[1]ì— ë¶™ì–´ì˜¤ëŠ” ì¼€ì´ìŠ¤ ëŒ€ì‘)
    # í•„ìš”í•˜ë©´ ì ‘ë‘ì–´ë¥¼ ë” ì¶”ê°€í•´ë„ ë¨.
    t = re.sub(r"^(ê³„ë¥˜ì˜ì•ˆ|ì²˜ë¦¬ì˜ì•ˆ)\s+", "", t)

    # âœ… ê´„í˜¸ ì „ë¶€ ì œê±° (ì—¬ëŸ¬ ê´„í˜¸ë„ ì‹¹ ì œê±°)
    t = re.sub(r"\s*\([^)]*\)", "", t)

    # ê³µë°± ì •ë¦¬
    t = re.sub(r"\s+", " ", t).strip()
    return t

# =========================================================
# 1) êµ­íšŒì˜ì› ì •ë³´ ìˆ˜ì§‘
# =========================================================
def collect_members():
    print("ğŸ” êµ­íšŒì˜ì› ì •ë³´ ìˆ˜ì§‘ ì‹œì‘")

    headers_members = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9",
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
        "Connection": "keep-alive",
    }

    # (ì´ì „ ì½”ë“œ ê·¸ëŒ€ë¡œ) - í•„ìš”í•˜ë©´ ë©¤ë²„ ì¶”ê°€/ìˆ˜ì •
    members = [
        ("ê¹€ì˜ì§„", "KIMYOUNGJIN"),
        ("ì •íƒœí˜¸", "JUNGTAEHO"),
        ("ê¹€ì˜í™˜", "KIMYOUNGWHAN"),
        ("ê¹€íƒœë…„", "KIMTAENYEON"),
        ("ë°•í™ê·¼", "PARKHONGKEUN"),
        ("ë°•ë¯¼ê·œ", "PARKMINKYU"),
        ("ì•ˆê·œë°±", "AHNGYUBACK"),
        ("ì•ˆë„ê±¸", "AHNDOGEOL"),
        ("ì˜¤ê¸°í˜•", "OHGIHYOUNG"),
        ("ì´ì†Œì˜", "LEESOYOUNG"),
        ("ì •ì„±í˜¸", "JUNGSUNGHO"),
        ("ì •ì¼ì˜", "CHUNGILYOUNG"),
        ("ì¡°ìŠ¹ë˜", "JOSEOUNGLAE"),
        ("ì§„ì„±ì¤€", "JINSUNGJOON"),
        ("ì†¡ì–¸ì„", "SONGEONSEOG"),
        ("ë°•ìˆ˜ì˜", "PARKSOOYOUNG"),
        ("ë°•ëŒ€ì¶œ", "PARKDAECHUL"),
        ("ë°•ì„±í›ˆ", "PARKSUNGHOON"),
        ("ìœ ìƒë²”", "YOOSANGBUM"),
        ("ìœ¤ì˜ì„", "YOONYOUNGSEOK"),
        ("ì´ì¸ì„ ", "LEEINSEON"),
        ("ì„ì´ì", "LIMLEEJA"),
        ("ìµœì€ì„", "CHOIEUNSEOK"),
        ("ì°¨ê·œê·¼", "CHAGYUGEUN"),
        ("ì²œí•˜ëŒ", "CHUNHARAM"),
        ("ìµœê¸°ìƒ", "CHOIKISANG"),
        ("ê¶Œì˜ì„¸", "KWONYOUNGSE"),
    ]

    party_mapping = {
        "ì •íƒœí˜¸": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
        "ê¹€ì˜ì§„": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
        "ê¹€ì˜í™˜": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
        "ê¹€íƒœë…„": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
        "ë°•í™ê·¼": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
        "ë°•ë¯¼ê·œ": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
        "ì•ˆê·œë°±": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
        "ì•ˆë„ê±¸": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
        "ì˜¤ê¸°í˜•": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
        "ì´ì†Œì˜": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
        "ì •ì„±í˜¸": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
        "ì •ì¼ì˜": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
        "ì¡°ìŠ¹ë˜": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
        "ì§„ì„±ì¤€": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
        "ìµœê¸°ìƒ": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
        "ì†¡ì–¸ì„": "êµ­ë¯¼ì˜í˜",
        "ë°•ìˆ˜ì˜": "êµ­ë¯¼ì˜í˜",
        "ë°•ëŒ€ì¶œ": "êµ­ë¯¼ì˜í˜",
        "ë°•ì„±í›ˆ": "êµ­ë¯¼ì˜í˜",
        "ìœ ìƒë²”": "êµ­ë¯¼ì˜í˜",
        "ìœ¤ì˜ì„": "êµ­ë¯¼ì˜í˜",
        "ì´ì¸ì„ ": "êµ­ë¯¼ì˜í˜",
        "ì„ì´ì": "êµ­ë¯¼ì˜í˜",
        "ìµœì€ì„": "êµ­ë¯¼ì˜í˜",
        "ê¶Œì˜ì„¸": "êµ­ë¯¼ì˜í˜",
        "ì°¨ê·œê·¼": "ì¡°êµ­í˜ì‹ ë‹¹",
        "ì²œí•˜ëŒ": "ê°œí˜ì‹ ë‹¹",
    }

    def extract_member_data(soup, fallback_name, member_id, status_code):
        name_el = soup.find("span", class_="sr-only")
        name = name_el.get_text(strip=True) if name_el else fallback_name

        party = party_mapping.get(name, "ì •ë³´ ì—†ìŒ")
        election_count, district, committee = "ì •ë³´ ì—†ìŒ", "ì •ë³´ ì—†ìŒ", "ì •ë³´ ì—†ìŒ"

        for dt in soup.find_all("dt"):
            label = dt.get_text(strip=True)
            dd = dt.find_next("dd")
            if not dd:
                continue
            val = dd.get_text(" ", strip=True)

            if label == "ë‹¹ì„ íšŸìˆ˜":
                election_count = val[:2]
            elif label == "ì„ ê±°êµ¬":
                district = val
            elif label == "ì†Œì†ìœ„ì›íšŒ":
                committee = val

        chief, senior, secretary = [], [], []
        for li in soup.find_all("li"):
            title = li.find("dt")
            value = li.find("dd")
            if not title or not value:
                continue
            role = title.get_text(strip=True)
            names = [n.strip() for n in value.get_text(strip=True).split(",") if n.strip()]
            if "ë³´ì¢Œê´€" in role:
                chief = names
            elif "ì„ ì„ë¹„ì„œê´€" in role:
                senior = names
            elif "ë¹„ì„œê´€" in role:
                secretary = names

        return {
            "êµ­íšŒì˜ì›": {
                "ì´ë¦„": name,
                "ì •ë‹¹": party,
                "ë‹¹ì„ íšŸìˆ˜": election_count,
                "ì„ ê±°êµ¬": district,
                "ì†Œì†ìœ„ì›íšŒ": committee,
            },
            "ë³´ì¢Œê´€": chief,
            "ì„ ì„ë¹„ì„œê´€": senior,
            "ë¹„ì„œê´€": secretary,
            "ë©”íƒ€ë°ì´í„°": {
                "url": f"https://www.assembly.go.kr/members/22nd/{member_id}",
                "status_code": status_code,
                "ìˆ˜ì§‘ì¼ì‹œ": now_kst_str(),
            },
        }

    session = requests.Session()
    session.headers.update(headers_members)

    all_member_data = []
    for name, member_id in members:
        url = f"https://www.assembly.go.kr/members/22nd/{member_id}"
        try:
            resp = session.get(url, timeout=30)
            soup = BeautifulSoup(resp.text, "html.parser")
            all_member_data.append(extract_member_data(soup, name, member_id, resp.status_code))
        except Exception as e:
            print(f"âš ï¸ [êµ­íšŒì˜ì›] {name} ì‹¤íŒ¨: {e}")
            all_member_data.append({
                "êµ­íšŒì˜ì›": {"ì´ë¦„": name, "ì •ë‹¹": party_mapping.get(name, "ì •ë³´ ì—†ìŒ"),
                          "ë‹¹ì„ íšŸìˆ˜": "ì •ë³´ ì—†ìŒ", "ì„ ê±°êµ¬": "ì •ë³´ ì—†ìŒ", "ì†Œì†ìœ„ì›íšŒ": "ì •ë³´ ì—†ìŒ"},
                "ë³´ì¢Œê´€": [], "ì„ ì„ë¹„ì„œê´€": [], "ë¹„ì„œê´€": [],
                "ë©”íƒ€ë°ì´í„°": {"url": url, "status_code": 0, "ìˆ˜ì§‘ì¼ì‹œ": now_kst_str()},
            })

    with open("assembly_member_data.json", "w", encoding="utf-8") as f:
        json.dump(all_member_data, f, ensure_ascii=False, indent=2)

    print("âœ… êµ­íšŒì˜ì› ì •ë³´ ì €ì¥ ì™„ë£Œ")


# =========================================================
# 2) ì˜ì•ˆì •ë³´(LIKMS) ìˆ˜ì§‘ - CSRF ìë™ + ì •í™• ì œëª© ë§¤ì¹­
# =========================================================
LIKMS_REFERER = "https://likms.assembly.go.kr/bill/bi/bill/sch/detailedSchPage.do"
LIKMS_FIND_URL = "https://likms.assembly.go.kr/bill/bi/bill/sch/findSchPaging.do"


def likms_prepare_session() -> requests.Session:
    """
    1) detailedSchPage.do GET í•´ì„œ ì¿ í‚¤/JSESSIONID/CSRF meta í™•ë³´
    2) ì´í›„ findSchPaging.do POSTì—ì„œ ê°™ì€ ì„¸ì…˜ ì‚¬ìš©
    """
    s = requests.Session()
    s.headers.update({
        "User-Agent": "Mozilla/5.0",
        "Referer": LIKMS_REFERER,
        "Origin": "https://likms.assembly.go.kr",
        "Accept-Language": "ko-KR,ko;q=0.9",
    })

    # CSRF ë©”íƒ€ê°€ ìˆì„ ìˆ˜ ìˆì–´ì„œ ë¨¼ì € referer í˜ì´ì§€ë¥¼ GET
    r = s.get(LIKMS_REFERER, timeout=30)
    r.raise_for_status()

    soup = BeautifulSoup(r.text, "html.parser")

    # ì¼ë°˜ì ìœ¼ë¡œ _csrf / _csrf_header ë©”íƒ€ê°€ ì¡´ì¬
    csrf = soup.find("meta", {"name": "_csrf"})
    csrf_header = soup.find("meta", {"name": "_csrf_header"})

    if csrf and csrf.get("content"):
        token = csrf["content"].strip()
        header_name = csrf_header["content"].strip() if (csrf_header and csrf_header.get("content")) else "X-CSRF-TOKEN"
        s.headers[header_name] = token

    # findSchPaging.doëŠ” form-urlencoded
    s.headers["Content-Type"] = "application/x-www-form-urlencoded;charset=UTF-8"
    s.headers["Accept"] = "text/html, */*; q=0.8"

    return s


def likms_fetch_by_billname(session: requests.Session, bill_name: str) -> str:
    """
    bill_nameë¡œ ê²€ìƒ‰ POST â†’ HTML(í…Œì´ë¸” í¬í•¨) ë°˜í™˜
    """
    data = {
        "reqPageId": "billSrch",
        "srchCmtId": "",
        "detailedTab": "billDtl",
        "gnStatsDiv": "",
        "srchBillDtlKindCd": "",
        "srchBillKindCd": "",
        "isGnStats": "",
        "dtlResultCd": "",
        "useNotIn": "",
        "mainQuery": "",
        "mainTabType": "",
        "fromMainBillStat": "",
        "billNm": bill_name,      # â˜… ì „ì²´ ì œëª© ê·¸ëŒ€ë¡œ ë„£ê³  ì•„ë˜ì—ì„œ ì •í™• ë§¤ì¹­
        "nmReSchText": "",
        "billNo": "",
        "representKindCd": "ì „ì²´",
        "represent": "",
        "representId": "",
        "isPopSelect": "N",
        "ageCmtId": "ì „ì²´",
        "ageFrom": "22",
        "ageTo": "22",
        "billKind": "ì „ì²´",
        "proposerKind": "ì „ì²´",
        "procGbnCd": "ì „ì²´",
        "jntPrpslYn": "ì „ì²´",
        "cmtResultCd": "ì „ì²´",
        "mainResultCd": "ì „ì²´",
        "mainUpdateYn": "ì „ì²´",
        "lawStatus": "ì „ì²´",
        "page": "1",
        "rows": "50",
        "schSorting": "score",
        "ordCd": "DESC",
    }

    r = session.post(LIKMS_FIND_URL, data=data, timeout=30)
    r.raise_for_status()
    return r.text


def likms_parse_and_filter(html: str, target_bill_name: str):
    """
    - í…Œì´ë¸”ì˜ tbody trì„ íŒŒì‹±
    - ì˜ì•ˆëª…(td[1])ì—ì„œë§Œ titleì„ ë½‘ì•„ì„œ normalize í›„ targetê³¼ ë™ì¼í•œ ê²ƒë§Œ ì €ì¥
    """
    soup = BeautifulSoup(html, "html.parser")
    table = soup.select_one("table")
    if not table:
        return [], []

    target_norm = normalize_bill_title(target_bill_name)

    all_titles_preview = []
    matched = []

    rows = table.select("tbody tr")
    for tr in rows:
        tds = tr.find_all("td")
        if len(tds) < 2:
            continue

        title_td = tds[1]
        raw_title = title_td.get_text(" ", strip=True)
        all_titles_preview.append(raw_title)

        norm_title = normalize_bill_title(raw_title)
        if norm_title != target_norm:
            continue

        bill_no = tds[0].get_text(strip=True)

        # billId ì¶”ì¶œ (onclick: fGoDetail('2212345', ...)
        bill_id = ""
        a = title_td.find("a")
        if a and a.has_attr("onclick"):
            m = re.search(r"fGoDetail\('([^']+)'", a["onclick"])
            if m:
                bill_id = m.group(1)

        proposer_kind = tds[2].get_text(strip=True) if len(tds) > 2 else ""
        propose_date = tds[3].get_text(strip=True) if len(tds) > 3 else ""
        vote_date = tds[4].get_text(strip=True) if len(tds) > 4 else ""
        vote_result = tds[5].get_text(strip=True) if len(tds) > 5 else ""
        status = tds[7].get_text(strip=True) if len(tds) > 7 else ""

        matched.append({
            "ì˜ì•ˆë²ˆí˜¸": bill_no,
            "ì˜ì•ˆID": bill_id,
            "ì˜ì•ˆëª…": raw_title,  # ì›ë¬¸ ìœ ì§€ (ê´„í˜¸ í¬í•¨)
            "ì œì•ˆìêµ¬ë¶„": proposer_kind,
            "ì œì•ˆì¼ì": propose_date,
            "ì˜ê²°ì¼ì": vote_date,
            "ì˜ê²°ê²°ê³¼": vote_result,
            "ì‹¬ì‚¬ì§„í–‰ìƒíƒœ": status,
            "ìƒì„¸URL": f"https://likms.assembly.go.kr/bill/bi/bill/detail.do?billId={bill_id}" if bill_id else "",
            "ìˆ˜ì§‘ì¼ì‹œ": now_kst_str(),
        })

    return matched, all_titles_preview


def collect_bills():
    print("ğŸ” ì˜ì•ˆ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘")

    bill_names = [
        "í•œêµ­ìˆ˜ì¶œì…ì€í–‰ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ",
        "ê²½ì œì•ˆë³´ë¥¼ ìœ„í•œ ê³µê¸‰ë§ ì•ˆì •í™” ì§€ì› ê¸°ë³¸ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ",
        "ì²¨ë‹¨ì¡°ì„ ì—…ì˜ ê²½ìŸë ¥ ê°•í™” ë° ì§€ì›ì— ê´€í•œ íŠ¹ë³„ë²•ì•ˆ",
        "ê³µê³µê¸°ê´€ì˜ ìš´ì˜ì— ê´€í•œ ë²•ë¥  ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ",
        "í•œêµ­ì‚°ì—…ì€í–‰ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ",
        "2025ë…„ë„ì— ë°œí–‰í•˜ëŠ” ì²¨ë‹¨ì „ëµì‚°ì—…ê¸°ê¸ˆì±„ê¶Œì— ëŒ€í•œ êµ­ê°€ë³´ì¦ë™ì˜ì•ˆ",
        "ì¤‘ì†Œê¸°ì—…ì€í–‰ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ",
        "ì •ë¶€ì¡°ì§ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ",
        "ì‹ ìš©ë³´ì¦ê¸°ê¸ˆë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ",
        "ë™ë‚¨ê¶Œì‚°ì—…íˆ¬ìê³µì‚¬ ì„¤ë¦½ ë° ìš´ì˜ì— ê´€í•œ ë²•ë¥ ì•ˆ",
        "ì¶©ì²­ê¶Œì‚°ì—…íˆ¬ìê³µì‚¬ ì„¤ë¦½ ë° ìš´ì˜ì— ê´€í•œ ë²•ë¥ ì•ˆ",
        "ê¸°í›„ìœ„ê¸° ëŒ€ì‘ì„ ìœ„í•œ íƒ„ì†Œì¤‘ë¦½ã†ë…¹ìƒ‰ì„±ì¥ ê¸°ë³¸ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ",
    ]

    session = likms_prepare_session()

    all_bills = []
    for name in bill_names:
        try:
            html = likms_fetch_by_billname(session, name)
            matched, preview = likms_parse_and_filter(html, name)

            if not preview:
                print(f"â„¹ï¸ [{name}] ê²€ìƒ‰ ê²°ê³¼(í–‰)ê°€ ì—†ìŠµë‹ˆë‹¤. (HTTP 200, í…Œì´ë¸”ì€ ìˆìœ¼ë‚˜ tbody ë¹„ì—ˆê±°ë‚˜ êµ¬ì¡°ë³€ê²½)")
            elif not matched:
                # ê²€ìƒ‰ì€ ë˜ëŠ”ë° ì •í™•ë§¤ì¹­ 0ê±´ì´ë©´ ì˜ˆì‹œ ì¶œë ¥
                ex = preview[:5]
                print(f"â„¹ï¸ [{name}] ê²€ìƒ‰ì€ ëì§€ë§Œ ì •í™•ì œëª© ë§¤ì¹­ 0ê±´. ì˜ˆì‹œ: {ex}")
            else:
                print(f"âœ… [{name}] {len(matched)}ê±´ ì €ì¥")

            all_bills.extend(matched)

        except Exception as e:
            print(f"âš ï¸ [ì˜ì•ˆ] {name} ìˆ˜ì§‘ ì‹¤íŒ¨: {type(e).__name__} - {e}")

    with open("ì˜ì•ˆì •ë³´ê²€ìƒ‰ê²°ê³¼.json", "w", encoding="utf-8") as f:
        json.dump(all_bills, f, ensure_ascii=False, indent=2)

    print(f"âœ… ì˜ì•ˆ ì •ë³´ ì €ì¥ ì™„ë£Œ: {len(all_bills)}ê±´")


# =========================================================
# 3) ì†Œìœ„ì›íšŒ ì •ë³´ ìˆ˜ì§‘
# =========================================================
def collect_subcommittees():
    print("ğŸ” ì†Œìœ„ì›íšŒ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘")

    session = requests.Session()
    main_url = "https://finance.na.go.kr:444/cmmit/mem/cmmitMemList/subCmt.do?menuNo=2000014"

    headers = {
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "ko-KR,ko;q=0.9",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
        "Pragma": "no-cache",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
    }

    main_resp = session.get(main_url, headers=headers, timeout=30)
    main_resp.raise_for_status()

    soup = BeautifulSoup(main_resp.text, "html.parser")
    csrf_parameter = soup.find("meta", {"name": "_csrf_parameter"})
    csrf_header = soup.find("meta", {"name": "_csrf_header"})
    csrf_token = soup.find("meta", {"name": "_csrf"})

    if not all([csrf_parameter, csrf_header, csrf_token]):
        raise RuntimeError("CSRF ë©”íƒ€ íƒœê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (finance.na.go.kr êµ¬ì¡° ë³€ê²½ ê°€ëŠ¥)")

    csrf_parameter_value = csrf_parameter["content"]
    csrf_header_value = csrf_header["content"]
    csrf_token_value = csrf_token["content"]

    api_headers = {
        "Accept": "application/json, text/javascript, */*; q=0.01",
        "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
        "Origin": "https://finance.na.go.kr:444",
        "Referer": main_url,
        "User-Agent": headers["User-Agent"],
        "X-Requested-With": "XMLHttpRequest",
        csrf_header_value: csrf_token_value,
    }

    data = {
        "hgNm": "",
        "subCmtNm": "",
        "pageUnit": "9",
        "pageIndex": "",
        csrf_parameter_value: csrf_token_value,
    }

    resp = session.post(
        "https://finance.na.go.kr:444/cmmit/mem/cmmitMemList/getSubCmitLst.json",
        headers=api_headers,
        data=data,
        timeout=30,
    )
    resp.raise_for_status()
    response_data = resp.json()

    def parse_members(member_str):
        members = []
        for m in member_str.split(","):
            m = m.strip()
            if m.startswith("â—ˆ"):
                m = m.lstrip("â—ˆ")
                if "(é•·)" not in m:
                    name_part, sep, han_part = m.partition("(")
                    m = f"(é•·){name_part}{sep}{han_part}"
            members.append(m)
        return members

    result = {}
    for item in response_data.get("resultList", []):
        committee_name = item.get("sbcmtNm", "")
        count = item.get("naasCnt", "")
        key = f"{committee_name}({count}ì¸)"

        parties = {}
        if item.get("poly1NaasNm") and item.get("poly1NaasCn"):
            parties[item["poly1NaasNm"]] = parse_members(item["poly1NaasCn"])
        if item.get("poly2NaasNm") and item.get("poly2NaasCn"):
            parties[item["poly2NaasNm"]] = parse_members(item["poly2NaasCn"])
        if item.get("poly99NaasNm") and item.get("poly99NaasCn"):
            parties[item["poly99NaasNm"]] = parse_members(item["poly99NaasCn"])

        result[key] = parties

    final_result = {
        "ì†Œìœ„ì›íšŒ_ì •ë³´": result,
        "ë©”íƒ€ë°ì´í„°": {
            "ìˆ˜ì§‘ì¼ì‹œ": now_kst_str(),
            "url": main_url,
            "status_code": resp.status_code,
        },
    }

    with open("ì†Œìœ„ì›íšŒì •ë³´.json", "w", encoding="utf-8") as f:
        json.dump(final_result, f, ensure_ascii=False, indent=2)

    print("âœ… ì†Œìœ„ì›íšŒ ì •ë³´ ì €ì¥ ì™„ë£Œ")


# =========================================================
# main
# =========================================================
def main():
    collect_members()
    collect_bills()
    collect_subcommittees()


if __name__ == "__main__":
    main()

