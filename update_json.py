import requests
from bs4 import BeautifulSoup
import json
import pandas as pd
from datetime import datetime
from zoneinfo import ZoneInfo

### =============== êµ­íšŒì˜ì› ì •ë³´ ìˆ˜ì§‘ ===============
cookies_members = {
    '_fwb': '224GIbJbv3W2VKbiHHpHIKa.1736910833680',
    '_ga': 'GA1.1.1553764147.1736910835',
    'PCID': '756e904c-0b66-a4a7-6836-589756f10a6e-1736910838628',
    'JSESSIONID': 'your_valid_session',
    'PHAROSVISITOR': '00006eb50196285f2f296cc80ac965a6',
}
headers_members = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9',
    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
    'Connection': 'keep-alive'
}
members = [
    ("ê¹€ì˜ì§„", "KIMYOUNGJIN"),
    ("ì •íƒœí˜¸", "JUNGTAEHO"),
    ("ê¹€ì˜í™˜", "KIMYOUNGWHAN"),
    ("ê¹€íƒœë…„", "KIMTAENYEON"),
    ("ë°•í™ê·¼", "PARKHONGKEUN"),
    ("ë°•ë¯¼ê·œ", "PARKMINKYU"),
    ("ì•ˆê·œë°±", "AHNGYUBACK"),
    ("ì•ˆë„ê±¸", "AHNDOGEOL"),
    ("ì˜¤ê¸°í˜•", "OHGIHYOUNG"),
    ("ì´ì†Œì˜", "LEESOYOUNG"),
    ("ì •ì„±í˜¸", "JUNGSUNGHO"),
    ("ì •ì¼ì˜", "CHUNGILYOUNG"),
    ("ì¡°ìŠ¹ë˜", "JOSEOUNGLAE"),
    ("ì§„ì„±ì¤€", "JINSUNGJOON"),
    ("ì†¡ì–¸ì„", "SONGEONSEOG"),
    ("ë°•ìˆ˜ì˜", "PARKSOOYOUNG"),
    ("ë°•ëŒ€ì¶œ", "PARKDAECHUL"),
    ("ë°•ì„±í›ˆ", "PARKSUNGHOON"),
    ("ìœ ìƒë²”", "YOOSANGBUM"),
    ("ìœ¤ì˜ì„", "YOONYOUNGSEOK"),
    ("ì´ì¸ì„ ", "LEEINSEON"),
    ("ì„ì´ì", "LIMLEEJA"),
    ("ìµœì€ì„", "CHOIEUNSEOK"),
    ("ì°¨ê·œê·¼", "CHAGYUGEUN"),
    ("ì²œí•˜ëŒ", "CHUNHARAM"),
    ("ìµœê¸°ìƒ", "CHOIKISANG"),
    ("ê¶Œì˜ì„¸", "KWONYOUNGSE"),

]

# ì •ë‹¹ ì •ë³´ ë§¤í•‘
party_mapping = {
    "ì •íƒœí˜¸": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ê¹€ì˜ì§„": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ê¹€ì˜í™˜": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ê¹€íƒœë…„": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ë°•í™ê·¼": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ë°•ë¯¼ê·œ": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ì•ˆê·œë°±": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ì•ˆë„ê±¸": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ì˜¤ê¸°í˜•": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ì´ì†Œì˜": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ì •ì„±í˜¸": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ì •ì¼ì˜": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ì¡°ìŠ¹ë˜": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ì§„ì„±ì¤€": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ìµœê¸°ìƒ": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ì†¡ì–¸ì„": "êµ­ë¯¼ì˜í˜",
    "ë°•ìˆ˜ì˜": "êµ­ë¯¼ì˜í˜",
    "ë°•ëŒ€ì¶œ": "êµ­ë¯¼ì˜í˜",
    "ë°•ì„±í›ˆ": "êµ­ë¯¼ì˜í˜",
    "ìœ ìƒë²”": "êµ­ë¯¼ì˜í˜",
    "ìœ¤ì˜ì„": "êµ­ë¯¼ì˜í˜",
    "ì´ì¸ì„ ": "êµ­ë¯¼ì˜í˜",
    "ì„ì´ì": "êµ­ë¯¼ì˜í˜",
    "ìµœì€ì„": "êµ­ë¯¼ì˜í˜",
    "ê¶Œì˜ì„¸": "êµ­ë¯¼ì˜í˜",
    "ì°¨ê·œê·¼": "ì¡°êµ­í˜ì‹ ë‹¹",
    "ì²œí•˜ëŒ": "ê°œí˜ì‹ ë‹¹"
}




def extract_member_data(soup, name, member_id, response):
    try:
        name_el = soup.find('span', class_='sr-only')
        name = name_el.text.strip() if name_el else name
        party = party_mapping.get(name, "ì •ë³´ ì—†ìŒ")
        election_count, district, committee = "ì •ë³´ ì—†ìŒ", "ì •ë³´ ì—†ìŒ", "ì •ë³´ ì—†ìŒ"
        for dt in soup.find_all('dt'):
            if dt.text == 'ë‹¹ì„ íšŸìˆ˜':
                election_count = dt.find_next('dd').text.strip()[:2]
            elif dt.text == 'ì„ ê±°êµ¬':
                district = dt.find_next('dd').text.strip()
            elif dt.text == 'ì†Œì†ìœ„ì›íšŒ':
                committee = dt.find_next('dd').text.strip()
        chief, senior, secretary = [], [], []
        for li in soup.find_all('li'):
            title = li.find('dt')
            value = li.find('dd')
            if not title or not value:
                continue
            role = title.text.strip()
            names = [n.strip() for n in value.text.split(',')]
            if 'ë³´ì¢Œê´€' in role:
                chief = names
            elif 'ì„ ì„ë¹„ì„œê´€' in role:
                senior = names
            elif 'ë¹„ì„œê´€' in role:
                secretary = names
        return {
            "êµ­íšŒì˜ì›": {
                "ì´ë¦„": name,
                "ì •ë‹¹": party,
                "ë‹¹ì„ íšŸìˆ˜": election_count,
                "ì„ ê±°êµ¬": district,
                "ì†Œì†ìœ„ì›íšŒ": committee
            },
            "ë³´ì¢Œê´€": chief,
            "ì„ ì„ë¹„ì„œê´€": senior,
            "ë¹„ì„œê´€": secretary,
            "ë©”íƒ€ë°ì´í„°": {
                "url": f"https://www.assembly.go.kr/members/22nd/{member_id}",
                "status_code": response.status_code,
                "ìˆ˜ì§‘ì¼ì‹œ": datetime.now(ZoneInfo("Asia/Seoul")).strftime("%Y-%m-%d %H:%M:%S")
            }
        }
    except Exception as e:
        return {
            "êµ­íšŒì˜ì›": {
                "ì´ë¦„": name,
                "ì •ë‹¹": party_mapping.get(name, "ì •ë³´ ì—†ìŒ"),
                "ë‹¹ì„ íšŸìˆ˜": "ì •ë³´ ì—†ìŒ",
                "ì„ ê±°êµ¬": "ì •ë³´ ì—†ìŒ",
                "ì†Œì†ìœ„ì›íšŒ": "ì •ë³´ ì—†ìŒ"
            },
            "ë³´ì¢Œê´€": [], "ì„ ì„ë¹„ì„œê´€": [], "ë¹„ì„œê´€": [],
            "ë©”íƒ€ë°ì´í„°": {
                "url": f"https://www.assembly.go.kr/members/22nd/{member_id}",
                "status_code": 500,
                "ìˆ˜ì§‘ì¼ì‹œ": datetime.now(ZoneInfo("Asia/Seoul")).strftime("%Y-%m-%d %H:%M:%S")
            }
        }

all_member_data = []
for name, member_id in members:
    try:
        url = f'https://www.assembly.go.kr/members/22nd/{member_id}'
        response = requests.get(url, cookies=cookies_members, headers=headers_members)
        soup = BeautifulSoup(response.text, 'html.parser')
        data = extract_member_data(soup, name, member_id, response)
        all_member_data.append(data)
    except Exception as e:
        print(f"[ERROR] {name} failed: {e}")

with open('assembly_member_data.json', 'w', encoding='utf-8') as f:
    json.dump(all_member_data, f, ensure_ascii=False, indent=4)
print("âœ… êµ­íšŒì˜ì› ì •ë³´ ì €ì¥ ì™„ë£Œ")


# ### =============== ì˜ì•ˆ ì •ë³´ ìˆ˜ì§‘ ===============
import json
import re
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from zoneinfo import ZoneInfo

SEARCH_URL = "https://likms.assembly.go.kr/bill/bi/bill/sch/detailedSchPage.do"

def collect_bill_info(bill_name: str):
    """
    ìƒˆ ì˜ì•ˆì •ë³´ì‹œìŠ¤í…œ(ë¦¬ë‰´ì–¼) ê¸°ì¤€:
    - XHR/JSONì´ ì•„ë‹ˆë¼, detailedSchPage.doì—ì„œ 'ì™„ì„±ëœ HTML ëª©ë¡'ì´ ë‚´ë ¤ì˜´
    - ê²°ê³¼ í–‰: tr.mono
    - billId: a[onclick]ì˜ fGoDetail('billId', ...)ì—ì„œ ì¶”ì¶œ
    - í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì •ì±…: ì˜ì•ˆëª…ì´ bill_nameê³¼ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²ƒë§Œ ì €ì¥
    """
    session = requests.Session()
    session.headers.update({
        "User-Agent": "Mozilla/5.0",
        "Referer": SEARCH_URL,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "ko-KR,ko;q=0.9",
    })

    # ğŸ”¥ í•µì‹¬: ê²€ìƒ‰ì–´ëŠ” query param billName
    params = {"billName": bill_name}

    r = session.get(SEARCH_URL, params=params, timeout=30)
    r.raise_for_status()

    soup = BeautifulSoup(r.text, "html.parser")

    bills = []
    rows = soup.select("tr.mono")

    for row in rows:
        tds = row.find_all("td")
        if len(tds) < 2:
            continue

        # 0) ì˜ì•ˆë²ˆí˜¸ (title ì†ì„±ì— ë“¤ì–´ìˆëŠ” ê²½ìš°ê°€ ë§ìŒ)
        bill_no = (tds[0].get("title") or tds[0].get_text(strip=True) or "").strip()

        # 1) ì˜ì•ˆëª… + billId
        title_td = tds[1]
        bill_title = title_td.get_text(" ", strip=True)

        # âœ… í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ í•„í„°: ì •í™•íˆ ì´ ë²•ë¥ ëª…ë§Œ ì €ì¥
        if normalize_title(bill_title) != normalize_title(bill_name):
            continue

        bill_id = ""
        a = title_td.find("a")
        if a and a.has_attr("onclick"):
            m = re.search(r"fGoDetail\('([^']+)'", a["onclick"])
            if m:
                bill_id = m.group(1)

        # 2) ì œì•ˆìêµ¬ë¶„/ì œì•ˆì¼ì/ì‹¬ì‚¬ì§„í–‰ìƒíƒœ ë“±ì€ í™”ë©´ êµ¬ì„±ì— ë”°ë¼ td ìœ„ì¹˜ê°€ ë³€í•  ìˆ˜ ìˆì–´
        #    ì•ˆì •ì ìœ¼ë¡œëŠ” ì •ê·œì‹/í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ì¼ë¶€ë§Œ ì¶”ì¶œ
        full_text = row.get_text(" ", strip=True)

        propose_date = ""
        m_date = re.search(r"\b(20\d{2}-\d{2}-\d{2})\b", full_text)
        if m_date:
            propose_date = m_date.group(1)

        # ì‹¬ì‚¬ì§„í–‰ìƒíƒœ: ëŒ€í‘œ í‚¤ì›Œë“œë§Œ ìš°ì„  ë§¤ì¹­ (í•„ìš” ì‹œ ì¶”ê°€ ê°€ëŠ¥)
        status = ""
        for kw in ["ì†Œê´€ìœ„ì ‘ìˆ˜", "ì†Œê´€ìœ„ì‹¬ì‚¬", "ë³¸íšŒì˜ë¶€ì˜ì•ˆê±´", "ê³µí¬", "ëŒ€ì•ˆë°˜ì˜íê¸°", "ì›ì•ˆê°€ê²°", "íê¸°", "ì ‘ìˆ˜"]:
            if kw in full_text:
                status = kw
                break

        bills.append({
            "ì˜ì•ˆë²ˆí˜¸": bill_no,
            "ì˜ì•ˆID": bill_id,  # âœ… ì¶”ê°€(ê¶Œì¥): update_json ì¤‘ë³µì œê±°/ìƒì„¸ë§í¬ìš©
            "ì˜ì•ˆëª…": {
                "text": bill_title,
                "link": f"javascript:fGoDetail('{bill_id}', 'billSimpleSearch')" if bill_id else ""
            },
            "ì œì•ˆìêµ¬ë¶„": "",        # ìƒˆ UIì—ì„œ ê³ ì • ì»¬ëŸ¼ì´ ì•„ë‹ ìˆ˜ ìˆì–´ ìš°ì„  ë¹ˆê°’ ìœ ì§€
            "ì œì•ˆì¼ì": propose_date, # ê°€ëŠ¥í•œ ê²½ìš°ë§Œ ì±„ì›€
            "ì˜ê²°ì¼ì": "",
            "ì˜ê²°ê²°ê³¼": "",
            "ì£¼ìš”ë‚´ìš©": {            # ìƒˆ UIì—ì„œ popup ë°©ì‹ì´ ë‹¬ë¼ì¡Œì„ ê°€ëŠ¥ì„± ë†’ì•„ ìš°ì„  ë¹ˆ ë§í¬
                "text": "ì£¼ìš”ë‚´ìš© ë³´ê¸°",
                "link": ""
            },
            "ì‹¬ì‚¬ì§„í–‰ìƒíƒœ": status,
            "ìˆ˜ì§‘ì¼ì‹œ": datetime.now(ZoneInfo("Asia/Seoul")).strftime("%Y-%m-%d %H:%M:%S"),
        })

    return bills


bill_names = [
    "í•œêµ­ìˆ˜ì¶œì…ì€í–‰ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ",
    "ê²½ì œì•ˆë³´ë¥¼ ìœ„í•œ ê³µê¸‰ë§ ì•ˆì •í™” ì§€ì› ê¸°ë³¸ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ",
    "ì²¨ë‹¨ì¡°ì„ ì—…ì˜ ê²½ìŸë ¥ ê°•í™” ë° ì§€ì›ì— ê´€í•œ íŠ¹ë³„ë²•ì•ˆ",
    "ê³µê³µê¸°ê´€ì˜ ìš´ì˜ì— ê´€í•œ ë²•ë¥  ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ",
    "í•œêµ­ì‚°ì—…ì€í–‰ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ",
    "í•œë¯¸ ì „ëµì  íˆ¬ì ê´€ë¦¬ë¥¼ ìœ„í•œ íŠ¹ë³„ë²•ì•ˆ",
    "ì¤‘ì†Œê¸°ì—…ì€í–‰ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ",
    "ì •ë¶€ì¡°ì§ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ",
    "ì‹ ìš©ë³´ì¦ê¸°ê¸ˆë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ",
    "ë™ë‚¨ê¶Œì‚°ì—…íˆ¬ìê³µì‚¬ ì„¤ë¦½ ë° ìš´ì˜ì— ê´€í•œ ë²•ë¥ ì•ˆ",
    "ì¶©ì²­ê¶Œì‚°ì—…íˆ¬ìê³µì‚¬ ì„¤ë¦½ ë° ìš´ì˜ì— ê´€í•œ ë²•ë¥ ì•ˆ",
    "ê¸°í›„ìœ„ê¸° ëŒ€ì‘ì„ ìœ„í•œ íƒ„ì†Œì¤‘ë¦½ã†ë…¹ìƒ‰ì„±ì¥ ê¸°ë³¸ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ"
]

all_bills = []
for bill_name in bill_names:
    try:
        rows = collect_bill_info(bill_name)
        # âœ… í˜¹ì‹œ ê°™ì€ ë²•ë¥ ëª…ì´ ì¤‘ë³µìœ¼ë¡œ ë“¤ì–´ì˜¤ë©´(ê±°ì˜ ì—†ì§€ë§Œ) ì¤‘ë³µ ì œê±°
        #    (ì˜ì•ˆIDê°€ ìˆìœ¼ë©´ ê·¸ ê¸°ì¤€ìœ¼ë¡œ ì œê±°)
        all_bills.extend(rows)
        print(f"âœ… [{bill_name}] {len(rows)}ê±´ ìˆ˜ì§‘")
    except Exception as e:
        print(f"âš ï¸ [{bill_name}] ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

with open("ì˜ì•ˆì •ë³´ê²€ìƒ‰ê²°ê³¼.json", "w", encoding="utf-8") as f:
    json.dump(all_bills, f, ensure_ascii=False, indent=4)

print(f"âœ… ì˜ì•ˆ ì •ë³´ ì €ì¥ ì™„ë£Œ: {len(all_bills)}ê±´")


### =============== ì†Œìœ„ì›íšŒ ì •ë³´ ìˆ˜ì§‘ ===============
# ì„¸ì…˜ ê°ì²´ ìƒì„±
session = requests.Session()

# ë©”ì¸ í˜ì´ì§€ URL
main_url = 'https://finance.na.go.kr:444/cmmit/mem/cmmitMemList/subCmt.do?menuNo=2000014'

# í—¤ë” ì •ì˜
headers = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'ko-KR,ko;q=0.9',
    'Cache-Control': 'no-cache',
    'Connection': 'keep-alive',
    'Pragma': 'no-cache',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
}

try:
    # ë©”ì¸ í˜ì´ì§€ ìš”ì²­
    main_response = session.get(main_url, headers=headers)
    main_response.raise_for_status()

    # CSRF ë©”íƒ€ íƒœê·¸ ì¶”ì¶œ
    soup = BeautifulSoup(main_response.text, 'html.parser')
    csrf_parameter = soup.find('meta', {'name': '_csrf_parameter'})
    csrf_header = soup.find('meta', {'name': '_csrf_header'})
    csrf_token = soup.find('meta', {'name': '_csrf'})

    if not all([csrf_parameter, csrf_header, csrf_token]):
        raise ValueError("CSRF ë©”íƒ€ íƒœê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    csrf_parameter_value = csrf_parameter['content']
    csrf_header_value = csrf_header['content']
    csrf_token_value = csrf_token['content']

    # POST ìš”ì²­ í—¤ë” ì„¤ì •
    api_headers = {
        'Accept': 'application/json, text/javascript, */*; q=0.01',
        'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
        'Origin': 'https://finance.na.go.kr:444',
        'Referer': main_url,
        'User-Agent': headers['User-Agent'],
        'X-Requested-With': 'XMLHttpRequest',
        csrf_header_value: csrf_token_value
    }

    data = {
        'hgNm': '',
        'subCmtNm': '',
        'pageUnit': '9',
        'pageIndex': '',
        csrf_parameter_value: csrf_token_value
    }

    response = session.post(
        'https://finance.na.go.kr:444/cmmit/mem/cmmitMemList/getSubCmitLst.json',
        headers=api_headers,
        data=data
    )
    response.raise_for_status()
    response_data = response.json()

except requests.exceptions.RequestException as e:
    print(f"ìš”ì²­ ì‹¤íŒ¨: {e}")
    exit(1)
except json.JSONDecodeError as e:
    print(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
    print("ì‘ë‹µ ì›ë¬¸:", response.text)
    exit(1)
except Exception as e:
    print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
    exit(1)

# â—ˆì´ë¦„ â†’ ì´ë¦„(é•·)ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def parse_members(member_str):
    members = []
    for m in member_str.split(','):
        m = m.strip()
        if m.startswith('â—ˆ'):
            m = m.lstrip('â—ˆ')  # remove diamond
            if '(é•·)' not in m:
                name_part, sep, han_part = m.partition('(')
                m = f"(é•·){name_part}{sep}{han_part}"  # insert "(é•·)" before names
        members.append(m)
    return members

# ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ êµ¬ì„±
result = {}

for item in response_data.get("resultList", []):
    committee_name = item["sbcmtNm"]
    count = item["naasCnt"]
    key = f"{committee_name}({count}ì¸)"
    
    parties = {}
    if item.get("poly1NaasNm") and item.get("poly1NaasCn"):
        parties[item["poly1NaasNm"]] = parse_members(item["poly1NaasCn"])
    if item.get("poly2NaasNm") and item.get("poly2NaasCn"):
        parties[item["poly2NaasNm"]] = parse_members(item["poly2NaasCn"])
    if item.get("poly99NaasNm") and item.get("poly99NaasCn"):
        parties[item["poly99NaasNm"]] = parse_members(item["poly99NaasCn"])
    
    result[key] = parties

# ë©”íƒ€ë°ì´í„° êµ¬ì„±
metadata = {
    "ìˆ˜ì§‘ì¼ì‹œ": datetime.now(ZoneInfo("Asia/Seoul")).strftime("%Y-%m-%d %H:%M:%S"),
    "url": main_url,
    "status_code": response.status_code
}

# ìµœì¢… ê²°ê³¼ ì €ì¥
final_result = {
    "ì†Œìœ„ì›íšŒ_ì •ë³´": result,
    "ë©”íƒ€ë°ì´í„°": metadata
}

with open('ì†Œìœ„ì›íšŒì •ë³´.json', 'w', encoding='utf-8') as f:
    json.dump(final_result, f, ensure_ascii=False, indent=4)
print("âœ… ì†Œìœ„ì›íšŒ ì •ë³´ ì €ì¥ ì™„ë£Œ")






