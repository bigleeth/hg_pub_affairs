import requests
from bs4 import BeautifulSoup
import json
import pandas as pd
from datetime import datetime
from zoneinfo import ZoneInfo

### =============== êµ­íšŒì˜ì› ì •ë³´ ìˆ˜ì§‘ ===============
cookies_members = {
    '_fwb': '224GIbJbv3W2VKbiHHpHIKa.1736910833680',
    '_ga': 'GA1.1.1553764147.1736910835',
    'PCID': '756e904c-0b66-a4a7-6836-589756f10a6e-1736910838628',
    'JSESSIONID': 'your_valid_session',
    'PHAROSVISITOR': '00006eb50196285f2f296cc80ac965a6',
}
headers_members = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9',
    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
    'Connection': 'keep-alive'
}
members = [
    ("ê¹€ì˜ì§„", "KIMYOUNGJIN"),
    ("ì •íƒœí˜¸", "JUNGTAEHO"),
    ("ê¹€ì˜í™˜", "KIMYOUNGWHAN"),
    ("ê¹€íƒœë…„", "KIMTAENYEON"),
    ("ë°•í™ê·¼", "PARKHONGKEUN"),
    ("ë°•ë¯¼ê·œ", "PARKMINKYU"),
    ("ì•ˆê·œë°±", "AHNGYUBACK"),
    ("ì•ˆë„ê±¸", "AHNDOGEOL"),
    ("ì˜¤ê¸°í˜•", "OHGIHYOUNG"),
    ("ì´ì†Œì˜", "LEESOYOUNG"),
    ("ì •ì„±í˜¸", "JUNGSUNGHO"),
    ("ì •ì¼ì˜", "CHUNGILYOUNG"),
    ("ì¡°ìŠ¹ë˜", "JOSEOUNGLAE"),
    ("ì§„ì„±ì¤€", "JINSUNGJOON"),
    ("ì†¡ì–¸ì„", "SONGEONSEOG"),
    ("ë°•ìˆ˜ì˜", "PARKSOOYOUNG"),
    ("ë°•ëŒ€ì¶œ", "PARKDAECHUL"),
    ("ë°•ì„±í›ˆ", "PARKSUNGHOON"),
    ("ìœ ìƒë²”", "YOOSANGBUM"),
    ("ìœ¤ì˜ì„", "YOONYOUNGSEOK"),
    ("ì´ì¸ì„ ", "LEEINSEON"),
    ("ì„ì´ì", "LIMLEEJA"),
    ("ìµœì€ì„", "CHOIEUNSEOK"),
    ("ì°¨ê·œê·¼", "CHAGYUGEUN"),
    ("ì²œí•˜ëŒ", "CHUNHARAM"),
    ("ìµœê¸°ìƒ", "CHOIKISANG"),
    ("ê¶Œì˜ì„¸", "KWONYOUNGSE"),

]

# ì •ë‹¹ ì •ë³´ ë§¤í•‘
party_mapping = {
    "ì •íƒœí˜¸": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ê¹€ì˜ì§„": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ê¹€ì˜í™˜": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ê¹€íƒœë…„": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ë°•í™ê·¼": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ë°•ë¯¼ê·œ": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ì•ˆê·œë°±": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ì•ˆë„ê±¸": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ì˜¤ê¸°í˜•": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ì´ì†Œì˜": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ì •ì„±í˜¸": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ì •ì¼ì˜": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ì¡°ìŠ¹ë˜": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ì§„ì„±ì¤€": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ìµœê¸°ìƒ": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
    "ì†¡ì–¸ì„": "êµ­ë¯¼ì˜í˜",
    "ë°•ìˆ˜ì˜": "êµ­ë¯¼ì˜í˜",
    "ë°•ëŒ€ì¶œ": "êµ­ë¯¼ì˜í˜",
    "ë°•ì„±í›ˆ": "êµ­ë¯¼ì˜í˜",
    "ìœ ìƒë²”": "êµ­ë¯¼ì˜í˜",
    "ìœ¤ì˜ì„": "êµ­ë¯¼ì˜í˜",
    "ì´ì¸ì„ ": "êµ­ë¯¼ì˜í˜",
    "ì„ì´ì": "êµ­ë¯¼ì˜í˜",
    "ìµœì€ì„": "êµ­ë¯¼ì˜í˜",
    "ê¶Œì˜ì„¸": "êµ­ë¯¼ì˜í˜",
    "ì°¨ê·œê·¼": "ì¡°êµ­í˜ì‹ ë‹¹",
    "ì²œí•˜ëŒ": "ê°œí˜ì‹ ë‹¹"
}




def extract_member_data(soup, name, member_id, response):
    try:
        name_el = soup.find('span', class_='sr-only')
        name = name_el.text.strip() if name_el else name
        party = party_mapping.get(name, "ì •ë³´ ì—†ìŒ")
        election_count, district, committee = "ì •ë³´ ì—†ìŒ", "ì •ë³´ ì—†ìŒ", "ì •ë³´ ì—†ìŒ"
        for dt in soup.find_all('dt'):
            if dt.text == 'ë‹¹ì„ íšŸìˆ˜':
                election_count = dt.find_next('dd').text.strip()[:2]
            elif dt.text == 'ì„ ê±°êµ¬':
                district = dt.find_next('dd').text.strip()
            elif dt.text == 'ì†Œì†ìœ„ì›íšŒ':
                committee = dt.find_next('dd').text.strip()
        chief, senior, secretary = [], [], []
        for li in soup.find_all('li'):
            title = li.find('dt')
            value = li.find('dd')
            if not title or not value:
                continue
            role = title.text.strip()
            names = [n.strip() for n in value.text.split(',')]
            if 'ë³´ì¢Œê´€' in role:
                chief = names
            elif 'ì„ ì„ë¹„ì„œê´€' in role:
                senior = names
            elif 'ë¹„ì„œê´€' in role:
                secretary = names
        return {
            "êµ­íšŒì˜ì›": {
                "ì´ë¦„": name,
                "ì •ë‹¹": party,
                "ë‹¹ì„ íšŸìˆ˜": election_count,
                "ì„ ê±°êµ¬": district,
                "ì†Œì†ìœ„ì›íšŒ": committee
            },
            "ë³´ì¢Œê´€": chief,
            "ì„ ì„ë¹„ì„œê´€": senior,
            "ë¹„ì„œê´€": secretary,
            "ë©”íƒ€ë°ì´í„°": {
                "url": f"https://www.assembly.go.kr/members/22nd/{member_id}",
                "status_code": response.status_code,
                "ìˆ˜ì§‘ì¼ì‹œ": datetime.now(ZoneInfo("Asia/Seoul")).strftime("%Y-%m-%d %H:%M:%S")
            }
        }
    except Exception as e:
        return {
            "êµ­íšŒì˜ì›": {
                "ì´ë¦„": name,
                "ì •ë‹¹": party_mapping.get(name, "ì •ë³´ ì—†ìŒ"),
                "ë‹¹ì„ íšŸìˆ˜": "ì •ë³´ ì—†ìŒ",
                "ì„ ê±°êµ¬": "ì •ë³´ ì—†ìŒ",
                "ì†Œì†ìœ„ì›íšŒ": "ì •ë³´ ì—†ìŒ"
            },
            "ë³´ì¢Œê´€": [], "ì„ ì„ë¹„ì„œê´€": [], "ë¹„ì„œê´€": [],
            "ë©”íƒ€ë°ì´í„°": {
                "url": f"https://www.assembly.go.kr/members/22nd/{member_id}",
                "status_code": 500,
                "ìˆ˜ì§‘ì¼ì‹œ": datetime.now(ZoneInfo("Asia/Seoul")).strftime("%Y-%m-%d %H:%M:%S")
            }
        }

all_member_data = []
for name, member_id in members:
    try:
        url = f'https://www.assembly.go.kr/members/22nd/{member_id}'
        response = requests.get(url, cookies=cookies_members, headers=headers_members)
        soup = BeautifulSoup(response.text, 'html.parser')
        data = extract_member_data(soup, name, member_id, response)
        all_member_data.append(data)
    except Exception as e:
        print(f"[ERROR] {name} failed: {e}")

with open('assembly_member_data.json', 'w', encoding='utf-8') as f:
    json.dump(all_member_data, f, ensure_ascii=False, indent=4)
print("âœ… êµ­íšŒì˜ì› ì •ë³´ ì €ì¥ ì™„ë£Œ")


# ### =============== ì˜ì•ˆ ì •ë³´ ìˆ˜ì§‘ ===============
import json
import re
import unicodedata
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from zoneinfo import ZoneInfo

SEARCH_URL = "https://likms.assembly.go.kr/bill/bi/bill/sch/detailedSchPage.do"

def normalize_bill_title(title: str) -> str:
    """
    - NFKC ì •ê·œí™” (íŠ¹ìˆ˜ ê³µë°±/ë¬¸ì ë³€í˜• ë°©ì§€)
    - ê´„í˜¸ ì´í›„ ì œê±°: "ë²•ë¥ ì•ˆ(â—‹â—‹ì˜ì› ë“±)" -> "ë²•ë¥ ì•ˆ"
    - ì–‘ë ê³µë°± ì œê±°
    """
    if not title:
        return ""
    t = unicodedata.normalize("NFKC", title).strip()
    t = re.split(r"\s*\(", t, maxsplit=1)[0].strip()
    return t

def collect_bill_info(bill_name: str):
    # 1. URLì„ ê¸°ì¡´ì— ì‚¬ìš©í•˜ë˜ 'ìƒì„¸ê²€ìƒ‰' í˜ì´ì§€ë¡œ ë³µêµ¬ (ì´ê²Œ ë§ëŠ” ì£¼ì†Œì…ë‹ˆë‹¤)
    URL = "https://likms.assembly.go.kr/bill/bi/bill/sch/detailedSchPage.do"
    
    session = requests.Session()
    session.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Referer": URL,
    })

    # 2. íŒŒë¼ë¯¸í„°ì— 'ëŒ€ìˆ˜(ageFrom)'ë¥¼ ì¶”ê°€í•˜ì—¬ 22ëŒ€ êµ­íšŒë¡œ í•œì • (ì •í™•ë„ í–¥ìƒ)
    params = {
        "billName": bill_name,
        "ageFrom": "22",  # 22ëŒ€ êµ­íšŒ
        "ageTo": "22",    # 22ëŒ€ êµ­íšŒ
        "searchGubun": "1" # ì˜ì•ˆëª… ê²€ìƒ‰
    }

    try:
        r = session.get(URL, params=params, timeout=30)
        r.raise_for_status()
    except Exception as e:
        print(f"âš ï¸ [ìš”ì²­ ì‹¤íŒ¨] {bill_name}: {e}")
        return []

    soup = BeautifulSoup(r.text, "html.parser")

    # 3. ìƒì„¸ê²€ìƒ‰ í˜ì´ì§€ì˜ í…Œì´ë¸” í–‰ ì„ íƒì (tr.mono ì‚¬ìš©)
    # ë§Œì•½ tr.monoê°€ ì•ˆ ë¨¹í ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ tbody ì•ˆì˜ ëª¨ë“  trì„ ê°€ì ¸ì˜´
    rows = soup.select("div.tableDisplay01 > table > tbody > tr")
    if not rows:
        rows = soup.select("tr.mono") # ê¸°ì¡´ ì„ íƒì ë°±ì—… ì‚¬ìš©

    if not rows:
        print(f"â„¹ï¸ [{bill_name}] ê²€ìƒ‰ ê²°ê³¼(í–‰)ê°€ ì—†ìŠµë‹ˆë‹¤. (URL ì ‘ì†ì€ ì„±ê³µ)")
        return []

    # ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ ë©”ì‹œì§€ ì²´í¬
    if len(rows) == 1 and ("ìë£Œê°€ ì—†ìŠµë‹ˆë‹¤" in rows[0].get_text() or "ê²€ìƒ‰ëœ ìë£Œê°€" in rows[0].get_text()):
        print(f"â„¹ï¸ [{bill_name}] í•´ë‹¹ í‚¤ì›Œë“œì— ëŒ€í•œ ì˜ì•ˆ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []

    target = normalize_bill_title(bill_name)
    print(f"ğŸ” [{bill_name}] ê²€ìƒ‰ ì„±ê³µ (ë°œê²¬: {len(rows)}ê±´)")

    bills = []
    
    for row in rows:
        tds = row.find_all("td")
        if len(tds) < 4:
            continue

        # ë°ì´í„° ì¶”ì¶œ (ìƒì„¸ê²€ìƒ‰ í˜ì´ì§€ êµ¬ì¡° ê¸°ì¤€)
        # ë²ˆí˜¸ | ì˜ì•ˆëª… | ì œì•ˆì | ì œì•ˆì¼ì | ì˜ê²°ê²°ê³¼ | ì‹¬ì‚¬ì§„í–‰ìƒíƒœ
        
        # 1. ì˜ì•ˆë²ˆí˜¸
        bill_no = tds[0].get_text(strip=True)

        # 2. ì˜ì•ˆëª… & ID
        title_td = tds[1]
        bill_title_full = title_td.get_text(" ", strip=True)
        
        bill_id = ""
        a = title_td.find("a")
        if a and a.has_attr("onclick"):
            # fGoDetail('PRC_...', 'detailedSchPage') íŒ¨í„´ ì¶”ì¶œ
            m = re.search(r"fGoDetail\(['\"]([^'\"]+)['\"]", a["onclick"])
            if m:
                bill_id = m.group(1)

        # 3. ì œì•ˆì
        proposer = tds[2].get_text(strip=True)
        
        # 4. ì œì•ˆì¼ì
        propose_date = tds[3].get_text(strip=True)
        
        # 5. ì‹¬ì‚¬ì§„í–‰ìƒíƒœ (ë³´í†µ 6ë²ˆì§¸ í˜¹ì€ ë§ˆì§€ë§‰ ì»¬ëŸ¼)
        status = tds[-1].get_text(strip=True)

        # ==========================================================
        # ğŸ”´ [í•µì‹¬ ìˆ˜ì •] í•„í„° ì™„í™” ë¡œì§
        # ì œëª©ì— ê²€ìƒ‰ì–´ê°€ 'í¬í•¨'ë˜ì–´ ìˆìœ¼ë©´ ìˆ˜ì§‘ (ê³µë°± ì œê±° í›„ ë¹„êµ)
        # ==========================================================
        clean_target = bill_name.replace(" ", "")
        clean_title = bill_title_full.replace(" ", "")
        
        if clean_target not in clean_title:
             # ë„ˆë¬´ ë‹¤ë¥¸ ì œëª©ì€ ê±´ë„ˆë›°ê¸°
             # print(f"   [Skip] ì œëª© ë¶ˆì¼ì¹˜: {bill_title_full}")
             continue

        print(f"   âœ… ìˆ˜ì§‘ ì„±ê³µ: {bill_title_full}")

        bills.append({
            "ì˜ì•ˆë²ˆí˜¸": bill_no,
            "ì˜ì•ˆID": bill_id,
            "ì˜ì•ˆëª…": {
                "text": bill_title_full,
                "link": f"https://likms.assembly.go.kr/bill/bi/bill/detail.do?billId={bill_id}" if bill_id else ""
            },
            "ì œì•ˆì": proposer,
            "ì œì•ˆì¼ì": propose_date,
            "ì‹¬ì‚¬ì§„í–‰ìƒíƒœ": status,
            "ìˆ˜ì§‘ì¼ì‹œ": datetime.now(ZoneInfo("Asia/Seoul")).strftime("%Y-%m-%d %H:%M:%S"),
        })

    return bills

def dump_debug(keyword: str, html: str):
    # Actions ë¡œê·¸ì— í•µì‹¬ë§Œ ì°ê¸°
    print(f"--- DEBUG HTML HEAD ({keyword}) ---")
    print(html[:1500])  # ì•ë¶€ë¶„
    print("... (snip) ...")
    print(html[-1500:]) # ë’·ë¶€ë¶„

    # HTML ì•ˆì— ìˆ¨ê²¨ì§„ API íŒíŠ¸(ajax url) ì°¾ê¸°
    for pat in ["findSch", "paging", "ajax", "search", "resultList", "X-Requested-With"]:
        if pat.lower() in html.lower():
            print(f"ğŸ” hint contains: {pat}")

if not rows:
    print(f"â„¹ï¸ [{keyword}] ê²€ìƒ‰ ê²°ê³¼(í–‰)ê°€ ì—†ìŠµë‹ˆë‹¤. (URL ì ‘ì†ì€ ì„±ê³µ)")
    dump_debug(keyword, r.text)
    return []


# âœ… ì´ ë²•ë¥ ë“¤ë§Œ ì €ì¥(ì‚¬ìš©ì ìš”êµ¬ ë°˜ì˜)
bill_names = [
    "í•œêµ­ìˆ˜ì¶œì…ì€í–‰ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ",
    "ê²½ì œì•ˆë³´ë¥¼ ìœ„í•œ ê³µê¸‰ë§ ì•ˆì •í™” ì§€ì› ê¸°ë³¸ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ",
    "ì²¨ë‹¨ì¡°ì„ ì—…ì˜ ê²½ìŸë ¥ ê°•í™” ë° ì§€ì›ì— ê´€í•œ íŠ¹ë³„ë²•ì•ˆ",
    "ê³µê³µê¸°ê´€ì˜ ìš´ì˜ì— ê´€í•œ ë²•ë¥  ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ",
    "í•œêµ­ì‚°ì—…ì€í–‰ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ",
    "2025ë…„ë„ì— ë°œí–‰í•˜ëŠ” ì²¨ë‹¨ì „ëµì‚°ì—…ê¸°ê¸ˆì±„ê¶Œì— ëŒ€í•œ êµ­ê°€ë³´ì¦ë™ì˜ì•ˆ",
    "ì¤‘ì†Œê¸°ì—…ì€í–‰ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ",
    "ì •ë¶€ì¡°ì§ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ",
    "ì‹ ìš©ë³´ì¦ê¸°ê¸ˆë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ",
    "ë™ë‚¨ê¶Œì‚°ì—…íˆ¬ìê³µì‚¬ ì„¤ë¦½ ë° ìš´ì˜ì— ê´€í•œ ë²•ë¥ ì•ˆ",
    "ì¶©ì²­ê¶Œì‚°ì—…íˆ¬ìê³µì‚¬ ì„¤ë¦½ ë° ìš´ì˜ì— ê´€í•œ ë²•ë¥ ì•ˆ",
    "ê¸°í›„ìœ„ê¸° ëŒ€ì‘ì„ ìœ„í•œ íƒ„ì†Œì¤‘ë¦½ã†ë…¹ìƒ‰ì„±ì¥ ê¸°ë³¸ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ"
]

all_bills = []
for name in bill_names:
    try:
        rows = collect_bill_info(name)
        all_bills.extend(rows)
        print(f"âœ… [{name}] {len(rows)}ê±´ ìˆ˜ì§‘")
    except Exception as e:
        print(f"âš ï¸ [{name}] ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

with open("ì˜ì•ˆì •ë³´ê²€ìƒ‰ê²°ê³¼.json", "w", encoding="utf-8") as f:
    json.dump(all_bills, f, ensure_ascii=False, indent=4)

print(f"âœ… ì˜ì•ˆ ì •ë³´ ì €ì¥ ì™„ë£Œ: {len(all_bills)}ê±´")


### =============== ì†Œìœ„ì›íšŒ ì •ë³´ ìˆ˜ì§‘ ===============
# ì„¸ì…˜ ê°ì²´ ìƒì„±
session = requests.Session()

# ë©”ì¸ í˜ì´ì§€ URL
main_url = 'https://finance.na.go.kr:444/cmmit/mem/cmmitMemList/subCmt.do?menuNo=2000014'

# í—¤ë” ì •ì˜
headers = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'ko-KR,ko;q=0.9',
    'Cache-Control': 'no-cache',
    'Connection': 'keep-alive',
    'Pragma': 'no-cache',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
}

try:
    # ë©”ì¸ í˜ì´ì§€ ìš”ì²­
    main_response = session.get(main_url, headers=headers)
    main_response.raise_for_status()

    # CSRF ë©”íƒ€ íƒœê·¸ ì¶”ì¶œ
    soup = BeautifulSoup(main_response.text, 'html.parser')
    csrf_parameter = soup.find('meta', {'name': '_csrf_parameter'})
    csrf_header = soup.find('meta', {'name': '_csrf_header'})
    csrf_token = soup.find('meta', {'name': '_csrf'})

    if not all([csrf_parameter, csrf_header, csrf_token]):
        raise ValueError("CSRF ë©”íƒ€ íƒœê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    csrf_parameter_value = csrf_parameter['content']
    csrf_header_value = csrf_header['content']
    csrf_token_value = csrf_token['content']

    # POST ìš”ì²­ í—¤ë” ì„¤ì •
    api_headers = {
        'Accept': 'application/json, text/javascript, */*; q=0.01',
        'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
        'Origin': 'https://finance.na.go.kr:444',
        'Referer': main_url,
        'User-Agent': headers['User-Agent'],
        'X-Requested-With': 'XMLHttpRequest',
        csrf_header_value: csrf_token_value
    }

    data = {
        'hgNm': '',
        'subCmtNm': '',
        'pageUnit': '9',
        'pageIndex': '',
        csrf_parameter_value: csrf_token_value
    }

    response = session.post(
        'https://finance.na.go.kr:444/cmmit/mem/cmmitMemList/getSubCmitLst.json',
        headers=api_headers,
        data=data
    )
    response.raise_for_status()
    response_data = response.json()

except requests.exceptions.RequestException as e:
    print(f"ìš”ì²­ ì‹¤íŒ¨: {e}")
    exit(1)
except json.JSONDecodeError as e:
    print(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
    print("ì‘ë‹µ ì›ë¬¸:", response.text)
    exit(1)
except Exception as e:
    print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
    exit(1)

# â—ˆì´ë¦„ â†’ ì´ë¦„(é•·)ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def parse_members(member_str):
    members = []
    for m in member_str.split(','):
        m = m.strip()
        if m.startswith('â—ˆ'):
            m = m.lstrip('â—ˆ')  # remove diamond
            if '(é•·)' not in m:
                name_part, sep, han_part = m.partition('(')
                m = f"(é•·){name_part}{sep}{han_part}"  # insert "(é•·)" before names
        members.append(m)
    return members

# ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ êµ¬ì„±
result = {}

for item in response_data.get("resultList", []):
    committee_name = item["sbcmtNm"]
    count = item["naasCnt"]
    key = f"{committee_name}({count}ì¸)"
    
    parties = {}
    if item.get("poly1NaasNm") and item.get("poly1NaasCn"):
        parties[item["poly1NaasNm"]] = parse_members(item["poly1NaasCn"])
    if item.get("poly2NaasNm") and item.get("poly2NaasCn"):
        parties[item["poly2NaasNm"]] = parse_members(item["poly2NaasCn"])
    if item.get("poly99NaasNm") and item.get("poly99NaasCn"):
        parties[item["poly99NaasNm"]] = parse_members(item["poly99NaasCn"])
    
    result[key] = parties

# ë©”íƒ€ë°ì´í„° êµ¬ì„±
metadata = {
    "ìˆ˜ì§‘ì¼ì‹œ": datetime.now(ZoneInfo("Asia/Seoul")).strftime("%Y-%m-%d %H:%M:%S"),
    "url": main_url,
    "status_code": response.status_code
}

# ìµœì¢… ê²°ê³¼ ì €ì¥
final_result = {
    "ì†Œìœ„ì›íšŒ_ì •ë³´": result,
    "ë©”íƒ€ë°ì´í„°": metadata
}

with open('ì†Œìœ„ì›íšŒì •ë³´.json', 'w', encoding='utf-8') as f:
    json.dump(final_result, f, ensure_ascii=False, indent=4)
print("âœ… ì†Œìœ„ì›íšŒ ì •ë³´ ì €ì¥ ì™„ë£Œ")











