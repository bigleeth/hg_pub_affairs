# news_scraper.py

import os
import urllib.request
import pandas as pd
import json
import re
from datetime import datetime, timedelta
import subprocess
import pytz

# === Naver OpenAPI credentials ===
client_id = "pfLKc2NgWoanoRnRDBgx"
client_secret = "efT6rgzJRG"

# === Keywords to search ===
keywords = [
    "ì´ì¬ëª…", "ì •ìƒìˆœë°©", "êµ­íšŒ", "ë³¸íšŒì˜", "ì¬ê²½ìœ„", "ì¬ì •ê²½ì œê¸°íšìœ„ì›íšŒ", "ì •ë¬´ìœ„", 
    "ì •íƒœí˜¸", "ê¹€ì˜ì§„ ì˜ì›", "ê¹€ì˜í™˜", "ê¹€íƒœë…„", "ë°•í™ê·¼", "ë°•ë¯¼ê·œ", "ì•ˆê·œë°±", "ì•ˆë„ê±¸", "ì˜¤ê¸°í˜•", "ì´ì†Œì˜ ì˜ì›", "ì •ì„±í˜¸", "ì •ì¼ì˜", "ì¡°ìŠ¹ë˜", "ì§„ì„±ì¤€", "ìµœê¸°ìƒ",
    "ì†¡ì–¸ì„", "ë°•ìˆ˜ì˜", "ë°•ëŒ€ì¶œ", "ë°•ì„±í›ˆ", "ìœ ìƒë²”", "ìœ¤ì˜ì„", "ì´ì¸ì„ ", "ì„ì´ì", "ìµœì€ì„", "ê¶Œì˜ì„¸", "ì°¨ê·œê·¼", "ì²œí•˜ëŒ",
    "ì˜¤ëŠ˜ì˜ ì£¼ìš”ì¼ì •", "ì˜¤ëŠ˜ì˜ êµ­íšŒì¼ì •", "ì„¸ì¢…í’í–¥ê³„", "ì„¸ì¢…25ì‹œ", "ê´€ê°€ëŠ” ì§€ê¸ˆ", "ê´€ê°€", "ê´€ë£Œ", "ê´€ê°€ë’·ë‹´", "ê´€ê°€ ì¸ì‚¬ì´ë“œ",
    "ì¬ê²½ë¶€", "ê¸°íšì²˜", "ê¸ˆìœµìœ„", "ìˆ˜ì¶œì…ì€í–‰", "ì‚°ì—…ì€í–‰", "ê¸°ì—…ì€í–‰", "ë¬´ì—­ë³´í—˜ê³µì‚¬",
    "ODA", "EDCF", "ê³µê¸‰ë§", "ëŒ€ë¯¸íˆ¬ì", "ì „ëµìˆ˜ì¶œê¸ˆìœµê¸°ê¸ˆ", "ë™ë‚¨ê¶Œíˆ¬ìê³µì‚¬", "ì§€ë°©ì´ì „", "ì´ì•¡ì¸ê±´ë¹„", "ë‚¨ë¶í˜‘ë ¥ê¸°ê¸ˆ"
]

# === Time range: from 18:00 yesterday to now ===
now = datetime.now(pytz.timezone('Asia/Seoul'))  # ì„œìš¸ íƒ€ì„ì¡´ìœ¼ë¡œ í˜„ì¬ ì‹œê°„ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
start_time = (now - timedelta(days=1)).replace(hour=18, minute=0, second=0, microsecond=0)

# === Function to clean HTML tags ===
def clean_html(text):
    return re.sub(re.compile('<.*?>'), '', text)

# === Create empty DataFrame ===
news_df = pd.DataFrame(columns=["Keyword", "Title", "Original Link", "Link", "Description", "Publication Date"])

# === Scrape Naver News API for each keyword ===
for keyword in keywords:
    query = urllib.parse.quote(keyword)
    display = 30
    start = 1
    sort = "date"

    url = f"https://openapi.naver.com/v1/search/news?query={query}&display={display}&start={start}&sort={sort}"

    request = urllib.request.Request(url)
    request.add_header("X-Naver-Client-Id", client_id)
    request.add_header("X-Naver-Client-Secret", client_secret)

    try:
        response = urllib.request.urlopen(request)
        if response.getcode() == 200:
            data = json.loads(response.read().decode('utf-8'))
            for item in data['items']:
                pub_date_str = item['pubDate']
                pub_datetime = datetime.strptime(pub_date_str, "%a, %d %b %Y %H:%M:%S %z")

                # ì‹œê°„ëŒ€ê°€ ìˆëŠ” pub_datetimeì™€ ë¹„êµ
                if pub_datetime >= start_time:
                    news_df.loc[len(news_df)] = [
                        keyword,
                        clean_html(item['title']),
                        item['originallink'],
                        item['link'],
                        clean_html(item['description']),
                        pub_date_str
                    ]
    except Exception as e:
        print(f"âŒ Error with keyword '{keyword}':", e)

# === Reorder columns ===
news_df = news_df[["Keyword", "Title", "Description", "Original Link", "Link", "Publication Date"]]

# === Save to local CSV file ===
output_dir = "data"
os.makedirs(output_dir, exist_ok=True)
today_str = now.strftime("%Y%m%d")
file_path = os.path.join(output_dir, f"pub_affair_articles.csv")

news_df.to_csv(file_path, index=False, encoding="utf-8-sig")
print(f"\nâœ… {len(news_df)} articles saved to: {file_path}")

# === Optional: Git auto-commit & push ===
auto_commit = True  # Set to False if you don't want git push

if auto_commit:
    try:
        subprocess.run(["git", "add", file_path], check=True)
        subprocess.run(["git", "commit", "-m", f"ğŸ“„ Auto-update articles on {today_str}"], check=True)
        subprocess.run(["git", "push"], check=True)
        print("ğŸ“¤ Changes pushed to GitHub.")
    except Exception as e:
        print("âŒ Git push failed:", e)
ã…ã…
